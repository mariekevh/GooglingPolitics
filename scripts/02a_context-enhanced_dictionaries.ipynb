{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-enhanced dictionary approach: Base dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('dutch') # is this the best stopword removal approach?\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing functions\n",
    "\n",
    "def transform_lowercase(x):\n",
    "    return x.lower()\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return re.sub(r'[^\\w\\s]|_', '', x)\n",
    "\n",
    "def remove_numbers(x):\n",
    "    return re.sub(r'\\d+', '', x)\n",
    "\n",
    "def remove_links(x):\n",
    "    return re.sub(r'http\\S+', '', x)\n",
    "\n",
    "def remove_linebreaks(x):\n",
    "    return x.replace('\\n', ' ').strip() # also remove double whitespace\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = x.split(\" \")\n",
    "    x = \" \".join([w for w in x if (w not in stopwords)&(w!=\"\")]) # if not stop word or empty\n",
    "    return x\n",
    "\n",
    "def list_of_words(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "def remove_numbers2(string):\n",
    "    pattern = r'\\b\\d+\\b'\n",
    "    matches = re.findall(pattern, string)\n",
    "    for match in matches:\n",
    "        string = string.replace(match, '')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_pickle(\"data/dictionaries/newsoutlets_100.pkl\")\n",
    "parties = pd.read_pickle(\"data/dictionaries/parties.pkl\")\n",
    "pol = pd.read_pickle(\"data/dictionaries/tweedekamerleden_kabinetsleden.pkl\")\n",
    "pa = pd.read_pickle(\"data/dictionaries/policy_agendas_dutch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polent = parties + pol # combine parties and politicians into political entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove because it messes things up\n",
    "polent = [w for w in polent if w not in ['50+', \"GO\", \"GOUD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Policy Agendas:',len(pa),\n",
    "      'News:', len(news),\n",
    "      'Parties:',len(parties),\n",
    "      'Politicians:', len(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with manual labels.\n",
    "labels = pd.read_csv(\"data/annotations.csv\")\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = labels.q.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "terms_lc = [transform_lowercase(w) for w in terms] # lowercase\n",
    "terms_lc_punc = [remove_punctuation(w) for w in terms_lc]# version with numbers in it\n",
    "terms_lc_num_punc = [remove_numbers2(w) for w in terms_lc_punc] # wihtout numbers, punctuation and lowercase\n",
    "terms_tokenized = [word_tokenize(w) for w in terms_lc_num_punc] # tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pa_lc_num_punc = []\n",
    "for w in pa:\n",
    "    w = transform_lowercase(w)\n",
    "    w = remove_punctuation(w)\n",
    "    w = remove_numbers2(w)\n",
    "    pa_lc_num_punc.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lc_num_punc = []\n",
    "for w in news:\n",
    "    w = transform_lowercase(w)\n",
    "    w = remove_numbers2(w)\n",
    "    w = remove_punctuation(w)\n",
    "    news_lc_num_punc.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polent_lc_num_punc = [] \n",
    "for w in polent:\n",
    "    w = transform_lowercase(w)\n",
    "    w = remove_punctuation(w)\n",
    "    w = remove_numbers2(w)\n",
    "    polent_lc_num_punc.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed versions\n",
    "terms_stemmed = [[stemmer.stem(w) for w in word] for word in terms_tokenized]\n",
    "pa_stemmed = [stemmer.stem(w) for w in pa_lc_num_punc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overlap with dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap(search_terms, dictionary):\n",
    "    results = []\n",
    "    for sublist in search_terms:\n",
    "        found = False\n",
    "        for word in sublist:\n",
    "            if word in dictionary:\n",
    "                found = True\n",
    "                break\n",
    "        results.append(1 if found else 0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlap2(search_terms, dictionary):\n",
    "    '''only whole words that match dictionary words, and not partial matches, by using word boundary characters (\\b).'''\n",
    "    pattern = r'\\b(' + '|'.join(dictionary) + r')\\b'\n",
    "    matches = []\n",
    "    for w in search_terms:\n",
    "        match = re.search(pattern, w)\n",
    "        matches.append(1 if match else 0)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_only_stemmed = check_overlap(terms_stemmed, pa_lc_num_punc) # pa is already stemmed\n",
    "pa_only_notstemmed = check_overlap(terms_tokenized, pa_lc_num_punc) # not stemmed\n",
    "news_only = check_overlap2(terms_lc_num_punc, news_lc_num_punc) # fully processed\n",
    "polent_only = check_overlap2(terms_lc_num_punc, polent_lc_num_punc) # fully processed\n",
    "news_word = check_overlap(terms_tokenized, ['nieuws']) # tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'pa_only_notstemmed':pa_only_notstemmed,\n",
    "                   'pa_only_stemmed':pa_only_stemmed,\n",
    "                   'news_only':news_only, \n",
    "                   'news_word':news_word, \n",
    "                   'polent_only':polent_only, \n",
    "                   'search_terms':terms, \n",
    "                   'terms_processed':terms_tokenized,\n",
    "                   'terms_stemmed':terms_stemmed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['news_only2'] = (df.news_only==1) | (df.news_word==1) # combine news word and news outlets.\n",
    "df['news_only2'] = df['news_only2'].astype(int)\n",
    "\n",
    "# combinations of dictionaries\n",
    "df['polent_news'] = (df.news_only2==1) | (df.polent_only==1)\n",
    "df['polent_news'] = df['polent_news'].astype(int)\n",
    "\n",
    "df['pa_notstemmed_news'] = (df.news_only2==1) | (df.pa_only_notstemmed==1)\n",
    "df['pa_notstemmed_news'] = df['pa_notstemmed_news'].astype(int)\n",
    "\n",
    "df['pa_stemmed_news'] = (df.news_only2==1) | (df.pa_only_stemmed==1)\n",
    "df['pa_stemmed_news'] = df['pa_stemmed_news'].astype(int)\n",
    "\n",
    "df['pa_notstemmed_polent'] = (df.polent_only==1) | (df.pa_only_notstemmed==1)\n",
    "df['pa_notstemmed_polent'] = df['pa_notstemmed_polent'].astype(int)\n",
    "\n",
    "df['pa_stemmed_polent'] = (df.polent_only==1) | (df.pa_only_stemmed==1)\n",
    "df['pa_stemmed_polent'] = df['pa_stemmed_polent'].astype(int)\n",
    "\n",
    "df['pa_notstemmed_polent_news'] = (df.polent_only==1) | (df.pa_only_notstemmed==1) | (df.news_only2==1)\n",
    "df['pa_notstemmed_polent_news'] = df['pa_notstemmed_polent_news'].astype(int)\n",
    "\n",
    "df['pa_stemmed_polent_news'] = (df.polent_only==1) | (df.pa_only_stemmed==1) | (df.news_only2==1)\n",
    "df['pa_stemmed_polent_news'] = df['pa_stemmed_polent_news'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(labels, df, left_on='q', right_on='search_terms')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "X_test = np.load(\"data/train_test/X_test.npy\", allow_pickle=True).tolist()\n",
    "y_test = np.load(\"data/train_test/y_test.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(x):\n",
    "    # for matching\n",
    "    return re.sub(r'[^\\w\\s]|_', '', x).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [preprocess2(x) for x in X_test]\n",
    "df['q_clean'] = df['q'].astype(str).apply(preprocess2)\n",
    "df2 = pd.DataFrame({\"test_set\":X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df2, df, left_on='test_set', right_on='q_clean', how='left')\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(df, cols):\n",
    "    '''\n",
    "    Takes list of classification reports as dicts as input, and outputs one table with only\n",
    "    '''\n",
    "    new = []\n",
    "    for c in cols:\n",
    "        dct = classification_report(df['Q1_checked'], df[c], output_dict=True)\n",
    "        dct = dct['1']\n",
    "        dct.update({'model':c})\n",
    "        new.append(dct)\n",
    "    new=pd.DataFrame(new).set_index('model')\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['polent_only', 'news_only2', 'pa_only_notstemmed', 'pa_only_stemmed', 'polent_news', \n",
    "        'pa_notstemmed_news', 'pa_stemmed_news', 'pa_notstemmed_polent', 'pa_stemmed_polent', \n",
    "        'pa_notstemmed_polent_news', 'pa_stemmed_polent_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = make_table(merged, cols)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in cols:\n",
    "    print(c)\n",
    "    print(classification_report(merged['Q1_checked'], merged[c]))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
