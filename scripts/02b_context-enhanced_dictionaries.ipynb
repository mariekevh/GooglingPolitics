{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98da100-1cae-477d-9846-c7590f1fe670",
   "metadata": {},
   "source": [
    "# Context-enhanced dictionaries: Rolling dictionaries\n",
    "Note that news content cannot be shared in this repo but is available publicly (see paper for source). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58bc9b-08f5-4f5d-b664-056742d5b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('dutch') \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer('dutch')\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15be21-1df5-4a64-b347-275faf3cb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing functions\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "def transform_lowercase(x):\n",
    "    return x.lower()\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return re.sub(r'[^\\w\\s]|_', '', x)\n",
    "\n",
    "def remove_numbers(x):\n",
    "    return re.sub(r'\\d+', '', x)\n",
    "\n",
    "def remove_numbers2(string): # avoid removing numbers when they are part of a word (e.g., D66)\n",
    "    pattern = r'\\b\\d+\\b'\n",
    "    matches = re.findall(pattern, string)\n",
    "    for match in matches:\n",
    "        string = string.replace(match, '')\n",
    "    return string.strip()\n",
    "\n",
    "def remove_links(x):\n",
    "    return re.sub(r'http\\S+', '', x)\n",
    "\n",
    "def remove_linebreaks(x):\n",
    "    return x.replace('\\n', ' ').strip() # also remove double whitespace\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = x.split(\" \")\n",
    "    x = \" \".join([w for w in x if (w not in stopwords)&(w!=\"\")]) # if not stop word or empty\n",
    "    return x\n",
    "\n",
    "def list_of_words(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words_stemmed = [stemmer.stem(word) for word in words]\n",
    "    return words_stemmed\n",
    "\n",
    "def extract_named_entities_nouns(text):\n",
    "    # Process whole documents\n",
    "    doc = nlp(text)\n",
    "    # Analyze syntax\n",
    "    named_entities = [chunk.text for chunk in doc.ents] # extract named entities\n",
    "    #print('named entities:', named_entities)\n",
    "    nouns = [chunk.text for chunk in doc if chunk.pos_ == 'NOUN']\n",
    "    #print('nouns:', nouns)\n",
    "    return list(set(named_entities + nouns))\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    # Process whole documents\n",
    "    doc = nlp(text)\n",
    "    # Analyze syntax\n",
    "    named_entities = [chunk.text for chunk in doc.ents] # extract named entities\n",
    "    #print('named entities:', named_entities)\n",
    "    return list(set(named_entities))\n",
    "\n",
    "\n",
    "\n",
    "def stem_list_of_words(x):\n",
    "    x = [stemmer.stem(w) for w in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77227c4b-a278-4e77-9e93-7144346cdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for matching purposes\n",
    "def preprocess(x):\n",
    "    # punctuation, lowercase, whitespace\n",
    "    return re.sub(r'[^\\w\\s]|_', '', x.lower()).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708eb5b6-d04a-4403-9fed-086c2511b6a2",
   "metadata": {},
   "source": [
    "# Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5651fd-4ddf-42e3-ae75-cea31207c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_pickle(\"data/dictionaries/newsoutlets_100.pkl\")\n",
    "parties = pd.read_pickle(\"Pdata/dictionaries/parties.pkl\")\n",
    "pol = pd.read_pickle(\"data/dictionaries/tweedekamerleden_kabinetsleden.pkl\")\n",
    "pa = pd.read_pickle(\"data/dictionaries/policy_agendas_dutch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd50b7-9ffa-4d7e-be93-9411dc4cad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "polent = parties + pol # combine parties and politicians into political entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0d664-52fe-4099-b017-846fca8b46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these because it messes things up\n",
    "polent = [w for w in polent if w not in ['50+', \"GO\", \"GOUD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30de1a-962e-4772-adcd-020266935970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Policy Agendas:',len(pa),\n",
    "      'News:', len(news),\n",
    "      'Parties:',len(parties),\n",
    "      'Politicians:', len(pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba5568-f136-415c-be95-e89922db23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d4913-8c23-46cf-85f0-14dc61c9738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lowercase, punct removal for policy agendas, simply lowercase for the rest\n",
    "news = [w.lower() for w in news]\n",
    "polent = [w.lower() for w in polent]\n",
    "pa = [remove_punctuation(w.lower()) for w in pa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e6423-f7cd-4340-8e56-9e161356a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news + [\"nieuws\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b07ee3-fa6b-4689-8792-903b48eb0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_list=polent+news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d3480-6dbe-4745-bda3-7d9ba65ad31e",
   "metadata": {},
   "source": [
    "# News article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772af720-f853-4d41-b067-d56d29aa7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"news_03012021-06302022.csv\")\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60a3cc-2fdc-4406-a53f-21d6024743ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20a70c-2eff-4084-9fc5-2fa741118766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime format\n",
    "news_data['date'] = pd.to_datetime(news_data['date'])\n",
    "#news.set_index('date', inplace=True)\n",
    "# Group the title column by date\n",
    "news_per_day = news_data.groupby(pd.Grouper(key='date', freq='D'))['title'].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed571a-dce4-4c91-b3b5-ddee33aada41",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303fbd1-48d7-47f0-98ed-df8f5a505bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nouns\n",
    "def preprocess_analysis_news(x, exclusion_list=exclusion_list):\n",
    "    x = extract_named_entities_nouns(x)\n",
    "    x = [w.lower() for w in x]\n",
    "    x = [remove_punctuation(w) for w in x if w not in exclusion_list]\n",
    "    x = [remove_numbers(w) for w in x if w not in exclusion_list] # remove punctuation but not if it's a party or news outlet name\n",
    "    x = [remove_links(e) for e in x]\n",
    "    x = [remove_linebreaks(e) for e in x]\n",
    "    x = [w.strip() for w in x]\n",
    "    x = [w for w in x if (w not in stopwords)&(w!=\"\")] # if not stop word or empty\n",
    "    return x\n",
    "\n",
    "#without nouns\n",
    "def preprocess_analysis_news2(x, exclusion_list=exclusion_list):\n",
    "    x = extract_named_entities(x)\n",
    "    x = [w.lower() for w in x]\n",
    "    x = [remove_punctuation(w) for w in x if w not in exclusion_list]\n",
    "    x = [remove_numbers(w) for w in x if w not in exclusion_list] # remove punctuation but not if it's a party or news outlet name\n",
    "    x = [remove_links(e) for e in x]\n",
    "    x = [remove_linebreaks(e) for e in x]\n",
    "    x = [w.strip() for w in x]\n",
    "    x = [w for w in x if (w not in stopwords)&(w!=\"\")] # if not stop word or empty\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddd419-91d5-41d0-a38b-32ae5a715313",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_per_day['title_c'] = news_per_day.title.apply(preprocess_analysis_news)\n",
    "news_per_day['title_c2'] = news_per_day.title.apply(preprocess_analysis_news2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964bb76-001e-4bca-b226-1957f1668e67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Search query data + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ae4d7-cab7-415b-9484-25e8d05cb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "X_test = np.load(\"data/train_test/X_test.npy\", allow_pickle=True).tolist()\n",
    "y_test = np.load(\"data/train_test/y_test.npy\", allow_pickle=True).tolist()\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7529fc-1dbe-44a0-860b-2261ed56eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"X_test\":X_test, \"y_test\":y_test})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba865a7-01dd-4183-b066-b9ec89622922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# browsing data\n",
    "br = pd.read_csv(\"browser2022.csv\")\n",
    "br.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2f086-35e8-4e99-93c5-d079150b8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search only\n",
    "br = br[br['text_search']==1].copy()\n",
    "br.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37668974-5f4e-4258-9c7b-55cb7edd92d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['date_easy'] = br.date_dt.astype(str).str[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f342b-a4e8-4a73-a6ca-a5ae9166a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make matching column\n",
    "df['q_match'] = df['X_test'].astype(str).apply(preprocess)\n",
    "br['q_match'] = br['q'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f8dfb-89d5-496c-8719-7cb1b40382f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,br,on='q_match', how='left')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eceed-9e46-4f5c-a1be-6ed7012496da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22,178 searches with labels.\n",
    "df.y_test.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f08ac2-8f19-4c28-a2e8-67d5a4fdf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.X_test.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48588e0-9d2a-4376-b057-6b3e41d76d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u = df.drop_duplicates(subset=['X_test', 'date_easy'])\n",
    "print(df.shape, df_u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2022f5-6c52-4296-a050-600cac589a5d",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478e4e6-f652-4ad4-bc47-9d75a6d3d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_analysis_sq(x, exclusion_list=exclusion_list):\n",
    "    x = transform_lowercase(x)\n",
    "    x = list_of_words(x)\n",
    "    x = [remove_punctuation(w) for w in x if w not in exclusion_list]\n",
    "    x = [remove_numbers(w) for w in x if w not in exclusion_list] # remove punctuation but not if it's a party or news outlet name\n",
    "    x = [w.strip() for w in x]\n",
    "    x = [w for w in x if (w not in stopwords)&(w!=\"\")]\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f7d12-9b2a-492f-a178-99a8b61e0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_u['X_test_c'] = df_u.X_test.apply(preprocess_analysis_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276699c-95ee-4db4-9648-c68999da532f",
   "metadata": {},
   "source": [
    "# News content overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185fd7f-ac30-4d03-bc35-11333b36c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeframe(given_date, days_before, days_after):\n",
    "    given_date = datetime.strptime(given_date, '%Y-%m-%d')\n",
    "    \n",
    "    dates_before = []\n",
    "    for i in range(1, days_before+1):\n",
    "        d = given_date - timedelta(days=i)\n",
    "        dates_before.append(d)\n",
    "        \n",
    "    dates_after = []\n",
    "    for i in range(1, days_after+1):\n",
    "        d = given_date + timedelta(days=i)\n",
    "        dates_after.append(d)\n",
    "    \n",
    "    timeframe = dates_before + [given_date] + dates_after\n",
    "    timeframe.sort()\n",
    "    \n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93742f-34de-4584-a17e-a5c57b2516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzymatching_news(query, query_date, news_df, text_col, days_before=0, days_after=1, threshold=80): \n",
    "    # slice news dataframe according to given timeframe.\n",
    "    timeframe = create_timeframe(query_date, days_before, days_after)\n",
    "\n",
    "    # slice news dataframe and return text\n",
    "    news_df = news_df[news_df['date'].isin(timeframe)]\n",
    "    title = news_df[text_col].tolist()\n",
    "    title = [item for sublist in title for item in sublist] # flatten list\n",
    "    \n",
    "    if isinstance(query, list):\n",
    "        query = \" \".join(query) # make one string\n",
    "    \n",
    "    for w in title:\n",
    "        r = fuzz.token_set_ratio(query, w)\n",
    "        if r >= threshold:\n",
    "            return 1\n",
    "    return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2f4d5-b96d-41cb-b07d-abb98f5ccadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_lst = df_u.X_test_c.tolist()\n",
    "d_lst = df_u.date_easy.tolist()\n",
    "l_lst = df_u.y_test.tolist()\n",
    "\n",
    "res = {\"query\":q_lst,\n",
    "       \"X_test\":df_u.X_test.tolist(),\n",
    "      \"date\":d_lst,\n",
    "      'label':l_lst,\n",
    "      }\n",
    "\n",
    "for t in [80,85,90,95]:\n",
    "    for col_name, col in zip(['entities', 'entities_nouns'], ['title_c2', 'title_c']):\n",
    "        col_name = col_name+\"_\"+str(t)\n",
    "        ratios = []\n",
    "        for query, query_date in tqdm(zip(q_lst, d_lst)):\n",
    "            ratio = fuzzymatching_news(query, query_date, news_per_day, col, days_before=1, days_after=1, threshold=t)\n",
    "            ratios.append(ratio)\n",
    "        res.update({col_name:ratios})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3157bf-a772-4a56-9fb3-8ef276762208",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5d8c9-9058-4d8a-9c18-c30c2871840c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## higher thresholds do not make a substantial difference\n",
    "cols_en = [c for c in res.columns if c.startswith(\"entities_nouns\")]\n",
    "for c in cols_en:\n",
    "    print(c)\n",
    "    print(classification_report(res['label'], res[c]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff3a00-ffde-4abb-a992-d0df8588efe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [c for c in res.columns if c.startswith(\"entities_\") and c not in cols_en]\n",
    "for c in cols:\n",
    "    print(c)\n",
    "    print(classification_report(res['label'], res[c]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d4fa0-307e-4826-8fca-906a981db32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(res['label'], res['entities_80']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab877df-abbf-47bf-821e-f6828e20d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(res['label'], res['entities_nouns_80']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea0488-7217-4205-91cd-9d4231ea6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities is better overall f1 and entities+nouns is better for recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2a05e-7130-4523-997f-360809370eef",
   "metadata": {},
   "source": [
    "## Combine with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484b674-d914-4674-afff-f896666975b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_preds = pd.read_csv(\"dictionary_preds.csv\")\n",
    "dictionary_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14462596-5f79-4cf2-8c0c-cb5b1d2c51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.merge(res, dictionary_preds[['X_test', 'pa_stemmed_polent_news_dict']], how='left', on='X_test')\n",
    "res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a00e7c-ece2-43fe-8296-a9f0ac2875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.pa_stemmed_polent_news_dict.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aca7ec-547d-4482-a11b-90babc7a080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.entities_80.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c26181-07d2-4af4-8958-283faa0f8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.entities_nouns_80.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5bf2c-d3dc-4a2a-8a3e-3e2c4bda320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2['dict_news_e'] = (res2['pa_stemmed_polent_news_dict'] | res2['entities_80']).astype(int)\n",
    "res2['dict_news_e_n'] = (res2['pa_stemmed_polent_news_dict'] | res2['entities_nouns_80']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59455bf-6a77-4448-8443-1f50795cccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.dict_news_e.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179f4fd-1de8-47e5-a946-f789b0967a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.dict_news_e_n.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc155a9e-21a0-4d72-bf60-074440d962ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(df, cols):\n",
    "    '''\n",
    "    Takes list of classification reports as dicts as input, and outputs one table with only\n",
    "    '''\n",
    "    new = []\n",
    "    for c in cols:\n",
    "        dct = classification_report(df['label'], df[c], output_dict=True)\n",
    "        dct = dct['1']\n",
    "        dct.update({'model':c})\n",
    "        new.append(dct)\n",
    "    new=pd.DataFrame(new).set_index('model')\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41064314-11f6-4140-8290-3cff9fa1bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"entities_80\", \"entities_nouns_80\", 'dict_news_e', 'dict_news_e_n']\n",
    "report = make_table(res2, cols)\n",
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
