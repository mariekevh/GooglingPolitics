{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144e28d3-af3b-4f28-acbb-16149c2c18ce",
   "metadata": {},
   "source": [
    "# Transformer-based SML: Multilingual (hyper)parameter search and evaluation of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72467eda-4574-4eeb-be78-2ea56d9ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bafd6c-7cca-4964-9c60-346a029cc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970c7c6-096c-4602-8448-8f4f91dbfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c388a-4295-4845-90bf-4caa3bfa8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('dutch') \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31019682-a557-4016-b70b-2b826a4aee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(x):\n",
    "    return re.sub(r'\\d+', '', x)\n",
    "\n",
    "def transform_lowercase(x):\n",
    "    return x.lower()\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return re.sub(r'[^\\w\\s]|_', '', x)\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    x = x.split(\" \")\n",
    "    x = \" \".join([w for w in x if (w not in stopwords)])\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = remove_punctuation(x)\n",
    "    x = remove_numbers(x)\n",
    "    x = transform_lowercase(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0c2fb-d300-4775-8d1e-7cb0085d92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-train split\n",
    "X_train = np.load(\"data/train_test/X_train.npy\", allow_pickle=True).tolist()\n",
    "X_test = np.load(\"data/train_test/X_test.npy\", allow_pickle=True).tolist()\n",
    "y_train = np.load(\"data/train_test/y_train.npy\", allow_pickle=True).tolist()\n",
    "y_test = np.load(\"data/train_test/y_test.npy\", allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122f82c-9391-4add-96aa-4511ffcb2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class balance in test and train data\n",
    "print('test data:', np.bincount(y_test))\n",
    "print('train data:', np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282a157-e464-4899-bf54-aecbcc6a7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase, punctuation and numbers removed\n",
    "X_train = [preprocess(w) for w in X_train]\n",
    "X_test = [preprocess(w) for w in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147250c-11ec-4f90-9ffe-dfb87002f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389111f-600f-4d1b-afc9-3843a3bd0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520fa7a-3b57-4166-a101-0066be158f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL OF CHOICE\n",
    "modelpath = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b973d-8cfd-48c1-97e8-838f2ddadc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = 'cuda' # = torch.device('cuda')\n",
    "max_length = 512 # This is the maximum number of tokens in any document sent to BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eac67c-7456-44ce-87f6-a2737a4c0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPARE DATA #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a901e5-c049-4f03-952b-ae53689ba8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a custom torch mydataset class to make a train_dataset object from the text and labels \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc7ea6-afa1-41c8-9611-82ffa7c5fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and val data set\n",
    "X_trains, X_val, y_trains, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18dae5-4b3f-427d-814d-036bad35ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set:', len(X_trains), np.bincount(y_trains))\n",
    "print('Validation set:', len(X_val), np.bincount(y_val))\n",
    "print('Test set:', len(X_test), np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b475-1767-4212-bf39-440df1768482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c7573-5fe6-4efa-8fd1-87c1d7a57870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "tokenizer = BertTokenizer.from_pretrained(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7135da5-47ba-48b0-a988-6bb1014335ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_trains, truncation=True, padding=True, max_length = max_length) \n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = max_length) \n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05af54b-4961-4e6f-8c8d-eb202b162ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_encodings, y_trains)\n",
    "val_dataset = MyDataset(val_encodings, y_val)\n",
    "test_dataset = MyDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f786f6-c876-4f33-b401-7152db35dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on relative importance of each class in the dataset (inverse frequency)\n",
    "class_weights1 = (1 - np.bincount(y_trains)/len(y_trains))\n",
    "class_weights1 = torch.from_numpy(class_weights1).float().to(\"cuda\")\n",
    "class_weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd6258-2198-4835-850c-8f4be159339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the weight. (different approach)\n",
    "class_weights2 = compute_class_weight(class_weight='balanced', classes=np.unique(y_trains), y=y_trains)\n",
    "class_weights2 = torch.from_numpy(class_weights2).float().to(\"cuda\")\n",
    "class_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac46eb-f49e-4b99-ad50-b8adb1863161",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43492255-39bb-45a4-921c-9ef367326293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom trainer based on class weights 1\n",
    "class WeightedLossTrainer1(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights1)\n",
    "        loss = loss_func(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fcf1c-6d8e-4e6b-9501-cbf393168787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLossTrainer2(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights2)\n",
    "        loss = loss_func(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b1a0b-f1ff-42e1-ae62-a6adc0c39940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    f1_1 = f1_score(labels, preds, pos_label=1)\n",
    "    prec_1 = precision_score(labels, preds, pos_label=1, zero_division=0) # divide by 0 but without printing warnings\n",
    "    rec_1 = recall_score(labels, preds, pos_label=1)\n",
    "\n",
    "    return {'macro_f1': macro_f1, 'f1' : f1, 'acc': acc, 'f1_1': f1_1, 'prec_1': prec_1, 'rec_1': rec_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e544c8-7bec-4124-8f57-b8d9ad5a78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(output):\n",
    "    params_df = pd.DataFrame([d['params'] for d in output])\n",
    "    df = pd.DataFrame(output).drop(columns='params')\n",
    "    final_df = pd.concat([params_df, df], axis=1)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0c20f-3ac2-4a01-8c1f-274ede649a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations I want to test.\n",
    "\n",
    "#do outside of loop: class weights, stop words, bert model\n",
    "grid = {'learning_rate': [5e-5],#, 3e-5, 2e-5], \n",
    "             'batch_size':[16],\n",
    "             'num_epochs':[3,4],\n",
    "             'warm_up':[0,1], #,1000],\n",
    "             'metric_name':['f1_1'] #['macro_f1']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896e05c-9959-40cf-abe6-305d346d87f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "n = 0\n",
    "\n",
    "for params in ParameterGrid(grid):\n",
    "    n+=1\n",
    "    # make sure to create a new folder\n",
    "    os.mkdir(f\"220323/multi/test{str(n)}\")\n",
    "    output_dir = f\"220323/multi/test{str(n)}/results\"\n",
    "    logging_dir = f\"220323/multi/test{str(n)}/logs\"\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(logging_dir)\n",
    "    \n",
    "    # make dictionary for performance scores\n",
    "    results = {'params':params}\n",
    "    \n",
    "    # (re)instiate model\n",
    "    model = BertForSequenceClassification.from_pretrained(modelpath, num_labels=2).to('cuda')\n",
    "    \n",
    "    # set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "    num_train_epochs=params['num_epochs'],              # total number of training epochs\n",
    "    per_device_train_batch_size=params['batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=params['batch_size'],   # batch size for evaluation\n",
    "    learning_rate=params['learning_rate'],  # initial learning rate for Adam optimizer\n",
    "    load_best_model_at_end=True,  # return best model after training\n",
    "    save_total_limit=2, # save max two checkpoints (best and last model in this case)\n",
    "    metric_for_best_model=params['metric_name'],             # best model evaluated on macro f1\n",
    "    warmup_steps=params['warm_up'],               # number of warmup steps for learning rate scheduler (set lower because of small dataset size) (default=0)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir=output_dir,          # output directory\n",
    "    logging_dir=logging_dir,            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps'     # evaluate during fine-tuning so that we can see progress\n",
    "    )\n",
    "    \n",
    "    # train train train\n",
    "    trainer = WeightedLossTrainer1(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # evaluating on validation set\n",
    "    temp = trainer.evaluate()\n",
    "    results.update(temp)\n",
    "    print('.....RESULTS....')\n",
    "    print(params)\n",
    "    print(results)\n",
    "    print('\\n\\n')\n",
    "    output.append(results)\n",
    "\n",
    "# save dictionary as pickle\n",
    "pickle.dump(output, open('220323/multi_weights1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e41c9-9bff-44c8-8d06-434ae00a3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df(output)\n",
    "df.sort_values('eval_f1_1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854210f-69d4-4646-a646-21634efa386e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output2 = []\n",
    "n = 49\n",
    "\n",
    "for params in ParameterGrid(grid):\n",
    "    n+=1\n",
    "    # make sure to create a new folder\n",
    "    os.mkdir(f\"220323/multi/test{str(n)}\")\n",
    "    output_dir = f\"220323/multi/test{str(n)}/results\"\n",
    "    logging_dir = f\"220323/multi/test{str(n)}/logs\"\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(logging_dir)\n",
    "    \n",
    "    # make dictionary for performance scores\n",
    "    results = {'params':params}\n",
    "    \n",
    "    # (re)instiate model\n",
    "    model = BertForSequenceClassification.from_pretrained(modelpath, num_labels=2).to('cuda')\n",
    "    \n",
    "    # set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "    num_train_epochs=params['num_epochs'],              # total number of training epochs\n",
    "    per_device_train_batch_size=params['batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=params['batch_size'],   # batch size for evaluation\n",
    "    learning_rate=params['learning_rate'],  # initial learning rate for Adam optimizer\n",
    "    load_best_model_at_end=True,  # return best model after training\n",
    "    save_total_limit=2, # save max two checkpoints (best and last model in this case)\n",
    "    metric_for_best_model=params['metric_name'],             # best model evaluated on macro f1\n",
    "    warmup_steps=params['warm_up'],               # number of warmup steps for learning rate scheduler (set lower because of small dataset size) (default=0)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir=output_dir,          # output directory\n",
    "    logging_dir=logging_dir,            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps'     # evaluate during fine-tuning so that we can see progress\n",
    "    )\n",
    "    \n",
    "    # train train train\n",
    "    trainer = WeightedLossTrainer2(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # evaluating on validation set\n",
    "    temp = trainer.evaluate()\n",
    "    results.update(temp)\n",
    "    print('.....RESULTS....')\n",
    "    print(params)\n",
    "    print(results)\n",
    "    print('\\n\\n')\n",
    "    output2.append(results)\n",
    "\n",
    "# save dictionary as pickle\n",
    "pickle.dump(output2, open('220323/multi_weights2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d3396-c2c8-478a-b75d-5232f76a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df(output2)\n",
    "df.sort_values('eval_f1_1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45968c65-13f5-4735-8bc2-74ba5d26b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## NO WEIGHTS \n",
    "\n",
    "output3 = []\n",
    "n = 79\n",
    "\n",
    "for params in ParameterGrid(grid):\n",
    "    n+=1\n",
    "    # make sure to create a new folder\n",
    "    os.mkdir(f\"220323/multi/test{str(n)}\")\n",
    "    output_dir = f\"220323/multi/test{str(n)}/results\"\n",
    "    logging_dir = f\"220323/multi/test{str(n)}/logs\"\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(logging_dir)\n",
    "    \n",
    "    # make dictionary for performance scores\n",
    "    results = {'params':params}\n",
    "    \n",
    "    # (re)instiate model\n",
    "    model = BertForSequenceClassification.from_pretrained(modelpath, num_labels=2).to('cuda')\n",
    "    \n",
    "    # set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "    num_train_epochs=params['num_epochs'],              # total number of training epochs\n",
    "    per_device_train_batch_size=params['batch_size'],  # batch size per device during training\n",
    "    per_device_eval_batch_size=params['batch_size'],   # batch size for evaluation\n",
    "    learning_rate=params['learning_rate'],  # initial learning rate for Adam optimizer\n",
    "    load_best_model_at_end=True,  # return best model after training\n",
    "    save_total_limit=2, # save max two checkpoints (best and last model in this case)\n",
    "    metric_for_best_model=params['metric_name'],             # best model evaluated on macro f1\n",
    "    warmup_steps=params['warm_up'],               # number of warmup steps for learning rate scheduler (set lower because of small dataset size) (default=0)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir=output_dir,          # output directory\n",
    "    logging_dir=logging_dir,            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps'     # evaluate during fine-tuning so that we can see progress\n",
    "    )\n",
    "    \n",
    "    # train train train\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        compute_metrics=compute_metrics)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # evaluating on validation set\n",
    "    temp = trainer.evaluate()\n",
    "    results.update(temp)\n",
    "    print('.....RESULTS....')\n",
    "    print(params)\n",
    "    print(results)\n",
    "    print('\\n\\n')\n",
    "    output3.append(results)\n",
    "\n",
    "# save dictionary as pickle\n",
    "pickle.dump(output3, open('220323/multi_noweights.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a3395-ee68-4c9e-a9f6-5bd41da37b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df(output3)\n",
    "df.sort_values('eval_f1_1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74601a-7259-4c8f-a082-d14ccfdff642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model based on f1 score -- test51 ---  (.62 f1, .72 prec, .54 rec): bs16, lr 5e-5, epochs 3, warmup steps 1, balanced class weights, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5573a48-35d0-41ad-ac94-8ee3672b44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and the arguments based on the validation set.\n",
    "best_path = \"220323/multi/test51/results/checkpoint-2000\"\n",
    "model = BertForSequenceClassification.from_pretrained(best_path)\n",
    "arguments = torch.load(f\"{best_path}/training_args.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d6782-c7fb-4a21-b482-947fd9bd0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedLossTrainer2(model=model, args=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcbffc-8f99-4683-9524-3fb05181386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data set\n",
    "preds = trainer.predict(test_dataset)\n",
    "print(preds.predictions.shape)\n",
    "predicted_labels = preds.predictions.argmax(-1) # Get the highest probability prediction\n",
    "predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1039f-8e47-4287-a498-6848f8b22bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the outcome\n",
    "print(classification_report(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf058b5-894a-4118-9388-13427962b7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
